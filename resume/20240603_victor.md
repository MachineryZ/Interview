Victor 面试问题

phd quit？

直播北大，放弃原因？想全身心搞量化相关事情。大四快结束就想做量化，怎么了解的？北大学生就业指导中心，工作接触到各行各业的信息多一些，觉得量化很适合。为什么没转院？大四的时候。

刚毕业cornell的时候拿了哪些 offer？国内offer出的比较早，比较纠结，国内量化，觉得国内前景很广阔。

Machine Learning Engine training：全自动的超参数调优，传统做法是一堆因子 fit y；mle是针对因子的子集和样本子集，去调小的model

超参搜索：human knowledge，以tree model 为例子，影响最大的超参，然后去搜索去不要搜索特别夸张的，深度是个位数，40，50没什么意义。尽量避免暴力的 grid search。没有循环调超参数，tree model 其实不太需要循环调参数，然后，人为经验放在一个还比较合理的数值范围内。单位时间单位算力里。

deep learning的调参区别，网络结果，缩减成最简单的形式？然后和线性模型很相似的。Fc + DenseLayer + seq layer + output layer，insample 

用 lightgbm 包

全公司大几十T，他个人用的服务器，6*3090，2T，96core，tree 没事，deep learning的采样？？流式数据，预留读几个batch。

Machine Learning Engine 大概开发了多久，没有说，代码量几千行。。。，半年多的开发周期。



Alpha Agent 的框架，挖掘、评估，搜索因子，map reduce的框架，处理多样本的中高频数据，输入是raw features和算子，然后生成各种各样的features。Multiple smart agents指的是：不同的 agent，把算子分成时序和非时序，有的agent看到时序+level 1，有的只能看到 所有feature 和 level 2。评估和筛选，因子库的概念，库里面有n个因子，新生成的k个，进行筛选，单因子ic和ir，筛选了一堆，嵌套了一个分布式的 ridge，ridge 的 yhat，搞一个加权的 r^2

1 搜索空间的问题，随机的搜索空间，生成 formula的时候控制深度，不能太复杂 2 排除完全不make sense 的 formula，没有特别严格，同一个算子反复叠加出现的套娃，但是好像没有很多规则上的限制 3 效率问题，现在这个框架用insample 2年，天并行，24小时能尝试接近百个因子，良率大概是千1；22年开始做，

tree model 选一个，del model 选大概1-2个；tree model，mle里面搜索的子集比较多，大model+小model bagging；deeplearning

为什么想跳槽，公司的工作氛围，不是很认真的在做事情。老板管理风格，各个环节隔绝，沟通比较。很多平台，还是比较大的，但是老板其实不希望每个员工涉及到太多。23年奖金发的也有问题



1. 原创机器学习因子挖掘算法？
2. 全自动 Machine Learning Engine
3. 因子挖掘框架 Alpha Agent？
4. 统计数据挖掘与机器学习项目里 Best Subset Selection Method 如何筛选因子
5. 随便讲一篇 NLP paper



-----



xgboost 的一些问题

### 1. 什么是XGBoost？

**回答：** XGBoost是一个优化的分布式梯度提升（Gradient Boosting）库，旨在提高机器学习算法的效率和速度。它以决策树为基础，通过对多个弱学习器（通常是决策树）的组合来创建一个强学习器。XGBoost以其速度和性能而闻名，特别是在大型数据集上。

### 2. XGBoost与传统的梯度提升（GBM）有什么区别？

**回答：** XGBoost与传统的GBM的主要区别在于以下几点：

- **正则化**：XGBoost引入了L1（Lasso）和L2（Ridge）正则化，以防止过拟合。
- **并行处理**：XGBoost支持并行计算，可以更快地训练模型。
- **树修剪**：XGBoost在分裂点选择之后进行树修剪，这样可以生成更简洁的树结构。
- **缺失值处理**：XGBoost能够自动处理缺失值，不需要预处理。
- **学习速率衰减**：XGBoost支持学习速率衰减（Shrinkage），通过减小每棵树的权重，逐步调整模型。
- **交叉验证**：XGBoost内置了交叉验证功能，方便进行模型评估。



在XGBoost中，L1和L2正则化是用来防止模型过拟合的技术。正则化通过在目标函数中添加惩罚项，约束模型的复杂度。具体来说：

- **L1正则化（Lasso）**：惩罚项是模型权重的绝对值和。
- **L2正则化（Ridge）**：惩罚项是模型权重的平方和。

### XGBoost中的正则化实现

在XGBoost中，正则化项被直接添加到目标函数中。对于决策树模型，目标函数的优化涉及到叶节点的分裂和权重的更新。具体实现步骤如下：

1. **构建树**：根据训练数据构建决策树。
2. **计算叶节点分裂增益**：对于每个可能的分裂点，计算分裂后的损失减少量（增益），同时考虑正则化项。分裂增益计算公式为：

$$\text{Gain} = \frac{1}{2} \left[ \frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} - \frac{(G_L + G_R)^2}{H_L + H_R + \lambda} \right] - \gamma $$

其中：
- $ G_L $ 和  $G_R$ 分别是左、右子节点的梯度和。
- \($ H_L $\) 和 \($ H_R $\) 分别是左、右子节点的二阶导数和（Hessian）。
- \( $\lambda$ \) 是L2正则化项的权重。
- \($ \gamma$ \) 是最小分裂损失减少量，用于控制分裂的保守性。

3. **更新叶节点权重**：在考虑正则化项的情况下，更新叶节点的权重。更新公式为：

$ w_j = -\frac{G_j}{H_j + \lambda} $

其中：
- $ G_j $ 是节点 $ j $ 的梯度和。
- $ H_j $ 是节点 $ j $ 的二阶导数和（Hessian）。
- $ \lambda $ 是L2正则化项的权重。

通过引入正则化项，XGBoost能够有效控制模型的复杂度，提高泛化能力，防止过拟合。



### 3. 什么是梯度提升（Gradient Boosting）？

**回答：** 梯度提升是一种提升方法，通过构建一系列弱学习器（如决策树），并逐步调整每个弱学习器以减少前一轮预测中的错误，从而构建一个强学习器。每一轮的学习器都是对前一轮错误的补偿，通过最小化损失函数的梯度来更新模型参数。

### 4. XGBoost如何处理缺失值？

**回答：** XGBoost能够自动处理缺失值。它会在训练过程中找到缺失值的最佳分裂方向。具体来说，对于每个分裂节点，XGBoost会评估将缺失值分配到左子树或右子树的增益，然后选择增益最大的分配方式。

### 5. XGBoost的超参数有哪些？请解释一些关键超参数。

**回答：** XGBoost有很多超参数，以下是一些关键超参数及其解释：

- **learning_rate（eta）**：控制每个弱学习器对模型的贡献，较小的学习率需要更多的弱学习器来训练。
- **n_estimators**：弱学习器（树）的数量。
- **max_depth**：树的最大深度，控制模型的复杂度。
- **min_child_weight**：控制子节点中最小的权重和，以防止过拟合。
- **subsample**：用于训练每棵树的样本比例，可以防止过拟合。
- **colsample_bytree**：用于训练每棵树的特征比例，可以防止过拟合。
- **gamma**：节点分裂所需的最小损失减少，值越大，算法越保守。
- **lambda（reg_lambda）**：L2正则化项的权重。
- **alpha（reg_alpha）**：L1正则化项的权重。



### 6. 解释XGBoost中的正则化项的作用。

**回答：** 正则化项的作用是防止模型过拟合。XGBoost使用L1和L2正则化：

- **L1正则化（Lasso）**：通过增加模型参数的绝对值的和来约束模型，促使一些系数变为零，进行特征选择。
- **L2正则化（Ridge）**：通过增加模型参数的平方和来约束模型，防止参数变得过大，从而降低模型的复杂度。

### 7. XGBoost如何进行并行计算？

**回答：** XGBoost通过在分裂节点时并行计算多个候选分裂点的增益来实现并行化。每个特征的所有可能分裂点都可以独立计算，因此可以并行进行，从而加速模型训练。此外，XGBoost在数据块和树结构的存储上也进行了优化，以支持并行处理。

### 8. 什么是树修剪（Tree Pruning）？

**回答：** 树修剪是减少决策树复杂度的技术。在XGBoost中，树修剪通过后剪枝（Post-Pruning）来实现。首先构建一个完全生长的树，然后从叶节点开始，计算如果移除该节点的增益是否小于给定阈值（gamma）。如果是，则移除该节点，从而简化树结构，防止过拟合。

### 9. 如何在XGBoost中调整超参数？

**回答：** 调整XGBoost的超参数通常需要以下步骤：

1. **固定基础参数**：如`n_estimators`和`learning_rate`，通常选择较小的学习率（如0.01或0.1），然后调整树的数量。
2. **调整树的结构参数**：如`max_depth`和`min_child_weight`，控制模型复杂度。
3. **调整子样本参数**：如`subsample`和`colsample_bytree`，防止过拟合。
4. **调整正则化参数**：如`gamma`、`lambda`和`alpha`，进一步控制模型复杂度。
5. **使用交叉验证**：使用`cv`函数进行交叉验证，选择最佳超参数组合。

### 10. 解释XGBoost中的损失函数。

**回答：** 在XGBoost中，损失函数用于衡量模型预测与真实值之间的差异。常见的损失函数包括：

- **均方误差（MSE）**：用于回归任务，最小化预测值和真实值之间的平方差。
- **对数损失（Log Loss）**：用于二分类任务，最小化预测概率与真实分类标签之间的差异。
- **多分类对数损失（Multiclass Log Loss）**：用于多分类任务，类似于对数损失，但扩展到多个类。

---

代码问题

 \_\_new\_\_ 和 \_\_init\_\_ 区别，new先调用返回一个 instance，init后调用负责初始化这个 instance；在singleton模式种确保只有一个 instance 被创建（相当于所有instance都是同一个，但是命名不一样罢了）

什么是yield，什么是生成器

什么是携程 coroutine

什么是死锁 

### 死锁的必要条件

死锁的产生必须满足以下四个条件，称为**Coffman条件**：

1. **互斥条件（Mutual Exclusion）**：
   - 至少有一个资源必须是非共享的，即资源一次只能被一个进程使用。
2. **占有和等待条件（Hold and Wait）**：
   - 一个进程已经占有了至少一个资源，并且在等待获取额外的资源，而这些资源被其他进程占有。
3. **不剥夺条件（No Preemption）**：
   - 资源不能被强制从进程中剥夺，资源只能由占有它的进程主动释放。
4. **环路等待条件（Circular Wait）**：
   - 存在一个进程链，使得链中的每一个进程都在等待链中下一个进程所占有的资源，从而形成一个环路。

**死锁预防（Prevention）**：

- **破坏互斥条件**：尽量避免资源的独占使用，尽可能地使用共享资源。
- **破坏占有和等待条件**：在进程请求资源时，如果资源不可用，不允许进程持有资源而等待，而是要求进程在持有资源前一次性申请所有需要的资源。
- **破坏不剥夺条件**：如果一个进程请求的资源被占用，可以强制剥夺其他进程的资源。
- **破坏环路等待条件**：对所有资源进行全局排序，并按顺序申请资源，避免形成环路。

**死锁避免（Avoidance）**：

- 通过某种算法动态地检查资源分配情况，以确保系统不会进入死锁状态。常用的方法包括银行家算法（Banker's Algorithm）。

**死锁检测和恢复（Detection and Recovery）**：

- 系统允许死锁发生，但会定期检测死锁，并采取措施进行恢复。常用的方法包括资源分配图和等待图。
- **恢复方法**：终止一个或多个进程以打破死锁，或回收资源并重新分配。

**鸵鸟策略（Ostrich Algorithm）**：

- 有些系统选择忽略死锁问题，假设死锁很少发生且影响不大，这种策略适用于对死锁容忍度较高的场景。

什么是虚拟内存





----

Alpha Agent

是强化学习的一种方法

----

为啥直博1年就准备申请出去了？