{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18808/281189703.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# check your forward result:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmymlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.01512607\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.89824537\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.46811376\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "# For this coding problem, we are about to simulate a deep learning \n",
    "# training process. \n",
    "# In computer vision field, classification is a very important\n",
    "# and traditional field. Assume that we have a some pseudo data input \n",
    "# for a mlp (multi-layer perceptron) model.\n",
    "\n",
    "# Q1: How should we arrange the sequence of fully-connected layer, activation\n",
    "# layer, batch normalization layer? Why?\n",
    "# x - relu - fc - bn\n",
    "# x - relu - bn - fc\n",
    "# x - bn - fc - relu\n",
    "# x - bn - relu - fc\n",
    "# x - fc - relu - bn\n",
    "# x - fc - bn - relu\n",
    "\n",
    "# Q2: What is the role for batch normalization playing for?\n",
    "\n",
    "# Q3: Assume we need to define a 2-layer mlp. \n",
    "# We have our pseudo data  x in shape of [batch_size, input_size]\n",
    "# We have our pseudo label y in shape of [batch_size, num_classes](one-hot vector)\n",
    "# We need you to !!!only!!! use numpy to build a training pipeline, \n",
    "# 1. Use Sigmoid as your activation function\n",
    "# 2. Use cross entropy loss\n",
    "# 3. No batch normalization\n",
    "# 4. Write your own back propogation progess to update your weights of fc layers.\n",
    "# 5. Using SGD optimizer algorithm\n",
    "# 6. Don't forget to do softmax in final output\n",
    "# 7. Finish the api we given to you\n",
    " \n",
    "import numpy as np\n",
    "\n",
    "# ============================== Question ==============================\n",
    "class MyMLP():\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        num_classes: int,\n",
    "        lr: float,\n",
    "        batch_size: int = 1,\n",
    "    ):\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.w1 = np.random.randn(hidden_size, input_size)\n",
    "        self.w2 = np.random.randn(num_classes, hidden_size)\n",
    "\n",
    "    def generate_data(self):\n",
    "        x = np.random.randn(self.batch_size, self.input_size) # [bs, input_size]\n",
    "        y = np.random.randint(0, self.num_classes, (self.batch_size, )) # [bs,]\n",
    "        y = np.eye(self.num_classes)[y] # [bs, num_cls]\n",
    "        return x, y\n",
    "\n",
    "    def sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "        # y = sigmoid(x)\n",
    "        return None\n",
    "    \n",
    "    def partial_sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "        # y' = d(sigmoid(x))/d(x)\n",
    "        return None\n",
    "\n",
    "    def softmax(self, x: np.ndarray) -> np.ndarray:\n",
    "        # x = [batch_size, dim] -> do softmax in column-wise dimension\n",
    "        return None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        # Two layer mlp\n",
    "        # x -> fc -> sigmoid -> fc -> softmax\n",
    "        # Notice that we use the formulation y = x W.T (without bias)  \n",
    "        self.a1 = None# fc1 output \n",
    "        self.h1 = None# sigmoid output \n",
    "        self.a2 = None# fc2 output \n",
    "        self.out = None# softmax output\n",
    "        return None\n",
    "    \n",
    "    def loss(self, y_hat: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        # yhat\n",
    "        # cross entropy loss\n",
    "        return None\n",
    "\n",
    "    def bp(self, x: np.ndarray, y:np.ndarray) -> np.ndarray:\n",
    "        # Update w1 and w2\n",
    "        self.dl_da2 = None # partial loss to partial a2\n",
    "        self.dl_dw2 = None# partial loss to partial w2\n",
    "        self.dl_dh1 = None# partial loss to partial h1\n",
    "        self.dl_da1 = None# partial loss to partial a1\n",
    "        self.d1_dw1 = None# partial loss to partial w1\n",
    "\n",
    "        # SGD Update w1 and w2:\n",
    "        self.w1 = None\n",
    "        self.w2 = None\n",
    "        return None\n",
    "\n",
    "    def matrix_differential_propogation(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        # Bonus\n",
    "        # Matrix-Matrix function derivatives\n",
    "        # Double check for gradient\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(2022)\n",
    "    mymlp = MyMLP(input_size=4, hidden_size=10, num_classes=3, lr=0.01, batch_size=1)\n",
    "    x, y = mymlp.generate_data()\n",
    "\n",
    "    # Step 1 \n",
    "    # check your forward result:\n",
    "    out = mymlp.forward(x)\n",
    "    assert np.linalg.norm(out - np.array([[0.36235674, 0.40728367, 0.23035959]])) < 1e-7\n",
    "\n",
    "    # Step 2 \n",
    "    # check your loss\n",
    "    loss = mymlp.loss(out, y)\n",
    "    assert np.linalg.norm(loss - np.array([[-1.01512607, -0. ,        -0.,        ]])) < 1e-7\n",
    "\n",
    "    w1, w2 = mymlp.bp(x, y)\n",
    "    # Step 2 \n",
    "    # check your gradients:\n",
    "    assert np.linalg.norm(mymlp.dl_da2 - np.array([[-0.63764326,  0.40728367,  0.23035959]])) < 1e-7\n",
    "    assert np.linalg.norm(mymlp.dl_dw2 - np.array([[-0.40732181, -0.47931036, -0.15217421, -0.03860381, -0.09738246, -0.36582423,\n",
    "        -0.57306758, -0.36619898, -0.53017721, -0.32491802],\n",
    "        [ 0.2601698,   0.30615125,  0.09719866,  0.02465752,  0.06220137,  0.23366394,\n",
    "        0.36603706,  0.2339033,   0.33864158,  0.2075358 ],\n",
    "        [ 0.14715201,  0.17315911,  0.05497555,  0.01394629,  0.03518109,  0.13216029,\n",
    "        0.20703051,  0.13229568,  0.19153563,  0.11738222]])) < 1e-7\n",
    "    assert np.linalg.norm(mymlp.dl_dh1 - np.array([[-0.22072836,  0.54220196,  0.2838984,  -0.24103344, -1.74668689, -0.65096623,\n",
    "        -0.33832413, -0.18948642,  0.76756801,  0.07541716]]))\n",
    "    assert np.linalg.norm(mymlp.dl_da1 - np.array([[-0.05093011,  0.10120303,  0.05158341, -0.01370905, -0.22601836, -0.15920446,\n",
    "        -0.03079302, -0.04632553,  0.1075607,   0.0188474 ]]))\n",
    "    assert np.linalg.norm(mymlp.dl_dw1 - np.array([[ 0.03771251, -0.04810758, -0.05592065, -0.02520829],\n",
    "        [-0.07493837,  0.09559438,  0.11111971,  0.05009129],\n",
    "        [-0.03819625,  0.04872466,  0.05663796,  0.02553164],\n",
    "        [ 0.01015122, -0.0129493,  -0.01505237, -0.00678541],\n",
    "        [ 0.16736108, -0.21349247, -0.24816545, -0.11186968],\n",
    "        [ 0.11788702, -0.15038138, -0.17480459, -0.07879958],\n",
    "        [ 0.02280148, -0.02908648, -0.03381036, -0.01524126],\n",
    "        [ 0.03430292, -0.04375818, -0.05086487, -0.02292921],\n",
    "        [-0.07964608,  0.10159971,  0.11810036,  0.05323807],\n",
    "        [-0.01395604,  0.01780288,  0.02069421,  0.00932868]])) < 1e-7\n",
    "\n",
    "    # Step 3\n",
    "    # check your updated weight result\n",
    "    assert np.linalg.norm(w1 - np.array([[-0.00090502, -0.27442035, -0.13872636,  1.98493824],\n",
    "            [ 0.28285871,  0.75985271,  0.29987041,  0.53979636],\n",
    "            [ 0.37387925,  0.37732615, -0.09077957, -2.30619859],\n",
    "            [ 1.14265851, -1.53552479, -0.86360149,  1.01661279],\n",
    "            [ 1.03229027, -0.8223573,   0.02138651, -0.38222486],\n",
    "            [-0.30536435,  0.99879532, -0.12552579, -1.47509791],\n",
    "            [-1.94113434,  0.83393979, -0.56687978,  1.17463937],\n",
    "            [ 0.3187258,   0.19130801,  0.36977883, -0.10091857],\n",
    "            [-0.94101303, -1.40515771,  2.07946601, -0.12084862],\n",
    "            [ 0.75993144,  1.82725411, -0.66093403, -0.80789955]])) < 1e-7\n",
    "    assert np.linalg.norm(w2 - np.array([[ 0.89187333, -0.21265435, -0.93800277,  0.59992435,  2.22408652,  1.0036637,\n",
    "        1.15540522, -0.1519143,  -1.64527408, -1.45186499],\n",
    "        [ 0.31815989,  0.80828812, -0.24177658,  0.16487451, -0.03412259,  0.08552794,\n",
    "        1.03048616, -1.06316559, -1.01697087, -0.42230754],\n",
    "        [ 0.93068173,  0.31559234, -0.943021,    0.32112784, -1.36990554, -0.21448964,\n",
    "        -0.11681657,  0.62104636,  0.55332894, -2.95860606]])) < 1e-7\n",
    "\n",
    "    dl_dw2 = mymlp.matrix_differential_propogation(x, y)\n",
    "    assert np.linalg.norm(dl_dw2 - np.array([[-0.40732181, -0.47931036, -0.15217421, -0.03860381, -0.09738246, -0.36582423,\n",
    "        -0.57306758, -0.36619898, -0.53017721, -0.32491802],\n",
    "        [ 0.2601698,   0.30615125,  0.09719866,  0.02465752,  0.06220137,  0.23366394,\n",
    "        0.36603706,  0.2339033,   0.33864158,  0.2075358 ],\n",
    "        [ 0.14715201,  0.17315911,  0.05497555,  0.01394629,  0.03518109,  0.13216029,\n",
    "        0.20703051,  0.13229568,  0.19153563,  0.11738222]])) < 1e-7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some reference for matrix derivates:\n",
    "   $$\n",
    "   \\begin{align}\n",
    "   &d(X\\pm Y) = dX \\pm dY,d(XY)=d XY+X dY \\\\\n",
    "   &d X\\odot Y=d X \\odot Y+X\\odot dY \\\\\n",
    "   &d(\\sigma(X))=\\sigma'(X)\\odot dX \\\\\n",
    "   &tr(AB) = tr(BA) \\\\\n",
    "   &tr(A^T(B\\odot C))=tr((A\\odot B)^TC), A,B,C\\in \\mathbb R^{n\\times n} \\\\\n",
    "   &df=tr(\\frac{\\partial f}{\\partial X}^T dX) \\\\\n",
    "   \\end{align}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\text{vec}(A + B)=\\text{vec}(A)+\\text{vec}(B) \\\\\n",
    "&\\text{vec}(AXB) = (B^T \\otimes A)\\text{vec}(X) \\\\\n",
    "&\\text{vec}(A^T)=K_{mn}\\text{vec}(A),A\\in \\mathbb R^{m\\times n},K_{mn}\\in \\mathbb R^{mn\\times mn}, \\quad K_{mn} \\text{  is commutation matrix} \\\\\n",
    "&\\text{vec}(A\\odot X)=\\text{dial}(A)\\text{vec}(X), \\text{\\quad diag}(A)\\in \\mathbb R^{mn\\times mn} \\text{  is a diagonal use elements in A, oredered by columns} \\\\\n",
    "&(A\\otimes B)^T=A^T\\otimes B^T \\\\\n",
    "&\\text{vec}(ab^T)=b\\otimes a \\\\\n",
    "&(A\\otimes B)(C\\otimes D)=(AC)\\otimes(BD) \\\\\n",
    "&K_{mn}=K^T_{nm}, K_{mn}K_{nm}=I \\\\\n",
    "&K_{pm}(A\\otimes B)K_{nq}=B\\otimes A, A\\in \\mathbb R^{m\\times n}, B\\in \\mathbb R^{p\\times q} \\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60294056b8f815b80b0581dc59f6047319bd3c1342d45a1f48b0c937a0336417"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('research': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
